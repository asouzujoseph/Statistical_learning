---
title: 'Regularized linear regression models - Ridge and Lasso regression '
output:
  html_document:
    df_print: paged
---

In this exercise, we will predict the number of applications received using the other variables in the College data set.


```{r}
library(glmnet)
library(ISLR)
library(caret)
```

```{r}
### dataset from the ISLR package
df <- College
df <- na.omit(df)
names(df)
```
```{r}
### create training and test dataset
set.seed(1000) # allows reproducibilty
index <- sample(1:nrow(df),0.8*nrow(df)) # use random sample (80%) as training data
train <- df[index,] # training dataset
test <- df[-index,] # test dataset
```


```{r}
### scale numeric features
cols <- c("Private","Accept","Enroll","Top10perc","Top25perc",
          "F.Undergrad","P.Undergrad","Outstate","Room.Board",
          "Books","Personal","PhD","Terminal","S.F.Ratio",
          "perc.alumni","Expend","Grad.Rate")
pre_proc <- preProcess(train[,cols], method =c("center", "scale"))
train[,cols] <- predict(pre_proc,train[,cols])
test[,cols] <- predict(pre_proc, test[,cols])
summary(train) # confirm that the mean of the predictor variables is zero

```

<br>

**Linear regression model**
```{r}
df.lm <- lm(Apps~., data = train)
summary(df.lm)

```
 The linear regression model is given as: <br>
*Apps = 3268.4 - 362.7(Private[Yes]) + 4155.6(Accept) - 936.1(enroll) + 674.1(Top10perc) -192.3(Top25perc) - 422.8(Outstate) + 167.8(Room.Board) + 541.2(expend) + 145.5(Grad.Rate) + e*
```{r}
## Predictions and performance of linear regression model
predictions <- predict(df.lm,test)
RMSE <- RMSE(predictions, test$Apps)
MSE <- RMSE(predictions, test$Apps)**2
data.frame(RMSE,MSE)
```

<br>
**Regularized linear models**
```{r}
### generate training and test data sets
set.seed(1000)
x_train <- model.matrix(Apps~.,train)[,-1] #predictor variables
y_train <- train$Apps # response variables
x_test <- model.matrix(Apps~.,test)[,-1]
y_test <- test$Apps
```

**Ridge regression**
```{r}
cv_ridge <- cv.glmnet(x_train, y_train, alpha = 0)
cv_ridge$lambda.min
ridge_model <- glmnet(x_train, y_train, alpha = 0, lambda = cv_ridge$lambda.min)
coef(ridge_model)
```
The ridge regression model is given as: <br>
*Apps = 3001.9 + 2636.2(Accept) + 377.2(Enroll) + 342.2(Top10perc) + 35.1(Top25perc) + 369.8(F.Undergrad) + 86.4(P.Undergrad) - 181.7(Outstate) + 198.5(Room.Board) + 12.2(Books) + 31.4(Personal) - 33.7(PhD) - 54.6(Terminal) + 75.1(S.F.ratio) -85.5(perc.alumni) + 482.8(Expend) +175(Grad.Rate) + e*

```{r}
## Predictions and performance of ridge regression model
predictions <- predict(ridge_model,x_test)
RMSE <- RMSE(predictions, y_test)
MSE <- RMSE(predictions, y_test)**2
data.frame(RMSE, MSE)
```
<br>
**Lasso regression**
```{r}
cv_lasso <- cv.glmnet(x_train, y_train, alpha = 1)
cv_lasso$lambda.min
lasso_model <- glmnet(x_train, y_train, alpha = 1, lambda = cv_lasso$lambda.min)
coef(lasso_model)
```

The lasso regression model is given as: <br>
*Apps = 3001.9 + 3840(Accept) - 282.5(Enroll) + 416.8(Top10perc) + 31.5(P.Undergrad) - 337.5(Outstate) + 112.7(Room.Board) + 31.1(Personal)- 65.9(PhD) - 21.3(Terminal) +39.7(S.F.Ratio) + 503.5(Expend) + 87.9(Grad.Rate) + e*

```{r}
## Predictions and performance of the lasso regression model
predictions <- predict(lasso_model,x_test)
RMSE <- RMSE(predictions, y_test)
MSE <- RMSE(predictions, y_test)**2
data.frame(RMSE, MSE)
```

The model performance assessment results show that ridge regression is the best moel compared to the linear and lasso regression models. This is because ridge regression does not discrad any of the predictor variables but rather reduces the coefficients of the predictor variables depending on their significance to the response. 

